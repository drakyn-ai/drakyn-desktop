# Inference engine: vllm or openai_compatible
INFERENCE_ENGINE=vllm

# OpenAI-compatible server URL (used when INFERENCE_ENGINE=openai_compatible)
OPENAI_COMPATIBLE_URL=http://localhost:11434

# Default model to load on startup (first time only)
# Can be a HuggingFace model name or local path
DEFAULT_MODEL=Qwen/Qwen2.5-0.5B-Instruct

# Currently selected model (auto-saved when you load a model)
# This persists your model selection across restarts
# CURRENT_MODEL=qwen2.5-coder:3b

# Whether to auto-load the model on server startup
AUTO_LOAD_MODEL=true

# GPU memory utilization (0.0 to 1.0)
# Lower this if you get OOM errors
GPU_MEMORY_UTILIZATION=0.9

# Server configuration
SERVER_HOST=127.0.0.1
SERVER_PORT=8000
